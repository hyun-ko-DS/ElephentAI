"""
ê²°ê³¼ ì •í™•ë„ ê³„ì‚°ê¸°
================

ì…ë ¥ ìŠ¤í™:
---------
1. CSV íŒŒì¼ ê²½ë¡œ (results í´ë” ë‚´ì˜ CSV íŒŒì¼)
   - test_folder: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë”ëª…
   - top_similar_predict: ê°€ì¥ ìœ ì‚¬í•˜ë‹¤ê³  ì˜ˆì¸¡í•œ í´ë”ëª…

ì¶œë ¥ ìŠ¤í™:
---------
1. ì „ì²´ ì •í™•ë„ (Accuracy)
2. í´ë”ë³„ ì •í™•ë„ ë¶„ì„
3. ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤ ìƒì„¸ ë¶„ì„
4. í˜¼ë™ í–‰ë ¬ (Confusion Matrix) í˜•íƒœì˜ ê²°ê³¼

ì‚¬ìš©ë²•:
------
python result_calculator.py results/ViT_g_14_result.csv
"""

import pandas as pd
import numpy as np
import os
import sys
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter

def load_results_csv(csv_path: str) -> pd.DataFrame:
    """
    ê²°ê³¼ CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
    
    ì¶œë ¥:
        pd.DataFrame: ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„
    """
    try:
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['test_folder', 'top_similar_predict']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
        
        print(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {csv_path}")
        print(f"ğŸ“Š ì´ {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
        print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
        
        return df
        
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def calculate_accuracy(df: pd.DataFrame) -> Dict[str, float]:
    """
    ì „ì²´ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        df (pd.DataFrame): ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    
    ì¶œë ¥:
        Dict[str, float]: ì •í™•ë„ ê´€ë ¨ ì§€í‘œë“¤
    """
    # ê²€ìƒ‰ ì‹¤íŒ¨ë‚˜ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì œì™¸
    valid_df = df[
        (df['top_similar_predict'] != 'ê²€ìƒ‰ ì‹¤íŒ¨') & 
        (~df['top_similar_predict'].str.startswith('ì˜¤ë¥˜:', na=False))
    ].copy()
    
    if len(valid_df) == 0:
        print("âš ï¸  ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    # ì •í™•ë„ ê³„ì‚°
    correct_predictions = (valid_df['test_folder'] == valid_df['top_similar_predict']).sum()
    total_predictions = len(valid_df)
    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
    
    # ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨ ê³„ì‚°
    failed_searches = len(df) - len(valid_df)
    failure_rate = failed_searches / len(df) if len(df) > 0 else 0.0
    
    results = {
        'total_samples': len(df),
        'valid_predictions': total_predictions,
        'correct_predictions': correct_predictions,
        'accuracy': accuracy,
        'failed_searches': failed_searches,
        'failure_rate': failure_rate
    }
    
    return results

def analyze_folder_accuracy(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    """
    í´ë”ë³„ ì •í™•ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        df (pd.DataFrame): ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    
    ì¶œë ¥:
        Dict[str, Dict[str, float]]: í´ë”ë³„ ì •í™•ë„ ì •ë³´
    """
    folder_accuracy = {}
    
    # ê²€ìƒ‰ ì‹¤íŒ¨ë‚˜ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì œì™¸
    valid_df = df[
        (df['top_similar_predict'] != 'ê²€ìƒ‰ ì‹¤íŒ¨') & 
        (~df['top_similar_predict'].str.startswith('ì˜¤ë¥˜:', na=False))
    ].copy()
    
    for folder in valid_df['test_folder'].unique():
        folder_data = valid_df[valid_df['test_folder'] == folder]
        
        if len(folder_data) == 0:
            continue
            
        correct = (folder_data['test_folder'] == folder_data['top_similar_predict']).sum()
        total = len(folder_data)
        accuracy = correct / total if total > 0 else 0.0
        
        folder_accuracy[folder] = {
            'total_samples': total,
            'correct_predictions': correct,
            'accuracy': accuracy,
            'incorrect_predictions': total - correct
        }
    
    return folder_accuracy

def analyze_misclassifications(df: pd.DataFrame) -> Dict[str, List[Dict]]:
    """
    ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        df (pd.DataFrame): ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    
    ì¶œë ¥:
        Dict[str, List[Dict]]: ì˜¤ë¶„ë¥˜ ë¶„ì„ ê²°ê³¼
    """
    # ê²€ìƒ‰ ì‹¤íŒ¨ë‚˜ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì œì™¸
    valid_df = df[
        (df['top_similar_predict'] != 'ê²€ìƒ‰ ì‹¤íŒ¨') & 
        (~df['top_similar_predict'].str.startswith('ì˜¤ë¥˜:', na=False))
    ].copy()
    
    # ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤
    misclassified = valid_df[valid_df['test_folder'] != valid_df['top_similar_predict']].copy()
    
    # ì‹¤ì œ í´ë”ë³„ë¡œ ì˜ëª» ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤ ê·¸ë£¹í™”
    misclass_by_actual = {}
    for _, row in misclassified.iterrows():
        actual_folder = row['test_folder']
        predicted_folder = row['top_similar_predict']
        
        if actual_folder not in misclass_by_actual:
            misclass_by_actual[actual_folder] = []
        
        misclass_by_actual[actual_folder].append({
            'test_filename': row['test_filename'],
            'predicted_folder': predicted_folder,
            'similarity_score': row.get('similarity_score', 'N/A')
        })
    
    return misclass_by_actual

def create_confusion_matrix(df: pd.DataFrame) -> pd.DataFrame:
    """
    í˜¼ë™ í–‰ë ¬ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        df (pd.DataFrame): ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    
    ì¶œë ¥:
        pd.DataFrame: í˜¼ë™ í–‰ë ¬
    """
    # ê²€ìƒ‰ ì‹¤íŒ¨ë‚˜ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì œì™¸
    valid_df = df[
        (df['top_similar_predict'] != 'ê²€ìƒ‰ ì‹¤íŒ¨') & 
        (~df['top_similar_predict'].str.startswith('ì˜¤ë¥˜:', na=False))
    ].copy()
    
    if len(valid_df) == 0:
        return pd.DataFrame()
    
    # í˜¼ë™ í–‰ë ¬ ìƒì„±
    confusion_matrix = pd.crosstab(
        valid_df['test_folder'], 
        valid_df['top_similar_predict'], 
        margins=True,
        margins_name='Total'
    )
    
    return confusion_matrix

def print_detailed_analysis(df: pd.DataFrame, csv_path: str) -> None:
    """
    ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        df (pd.DataFrame): ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
    """
    # print("\n" + "=" * 80)
    # print(f"ğŸ“Š ê²°ê³¼ ë¶„ì„: {os.path.basename(csv_path)}")
    # print("=" * 80)
    
    # 1. ì „ì²´ ì •í™•ë„
    accuracy_results = calculate_accuracy(df)
    if not accuracy_results:
        return
    
    print(f"\nğŸ¯ ì „ì²´ ì •í™•ë„ ë¶„ì„")
    print("-" * 50)
    print(f"ğŸ“ ì´ ìƒ˜í”Œ ìˆ˜: {accuracy_results['total_samples']:,}")
    print(f"âœ… ìœ íš¨í•œ ì˜ˆì¸¡: {accuracy_results['valid_predictions']:,}")
    print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨/ì˜¤ë¥˜: {accuracy_results['failed_searches']:,}")
    print(f"ğŸ“Š ì •í™•ë„: {accuracy_results['accuracy']:.4f} ({accuracy_results['accuracy']*100:.2f}%)")
    print(f"âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨: {accuracy_results['failure_rate']:.4f} ({accuracy_results['failure_rate']*100:.2f}%)")
    
    # 2. í´ë”ë³„ ì •í™•ë„
    folder_accuracy = analyze_folder_accuracy(df)
    if folder_accuracy:
        print(f"\nğŸ“‚ í´ë”ë³„ ì •í™•ë„ ë¶„ì„")
        print("-" * 50)
        
        # ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_folders = sorted(folder_accuracy.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        
        for folder, stats in sorted_folders:
            print(f"{folder}: {stats['accuracy']:.4f} ({stats['correct_predictions']}/{stats['total_samples']})")
    
    # 3. ì˜¤ë¶„ë¥˜ ë¶„ì„
    misclassifications = analyze_misclassifications(df)
    if misclassifications:
        print(f"\nâŒ ì˜¤ë¶„ë¥˜ ë¶„ì„")
        print("-" * 50)
        
        for actual_folder, cases in misclassifications.items():
            # print(f"\nğŸ” {actual_folder} í´ë”ì—ì„œ ì˜ëª» ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤:")
            
            # ì˜ˆì¸¡ëœ í´ë”ë³„ë¡œ ê·¸ë£¹í™”
            pred_counts = Counter([case['predicted_folder'] for case in cases])
            
            for pred_folder, count in pred_counts.most_common():
                # print(f"   â†’ {pred_folder}: {count}ê°œ")
                
                # ìƒì„¸ ì •ë³´ (ì²˜ìŒ 3ê°œë§Œ)
                detailed_cases = [case for case in cases if case['predicted_folder'] == pred_folder][:3]
                for case in detailed_cases:
                    similarity = case.get('similarity_score', 'N/A')
                    # if isinstance(similarity, (int, float)):
                        # print(f"     - {case['test_filename']} (ìœ ì‚¬ë„: {similarity:.4f})")
                    # else:
                        # print(f"     - {case['test_filename']} (ìœ ì‚¬ë„: {similarity})")
    
    # 4. í˜¼ë™ í–‰ë ¬
    confusion_matrix = create_confusion_matrix(df)
    if not confusion_matrix.empty:
        print(f"\nğŸ“‹ í˜¼ë™ í–‰ë ¬ (Confusion Matrix)")
        print("-" * 50)
        print(confusion_matrix.to_string())
    
    # 5. ìš”ì•½
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½")
    print("-" * 50)
    print(f"ğŸ¯ ì „ì²´ ì •í™•ë„: {accuracy_results['accuracy']*100:.2f}%")
    print(f"ğŸ“ ë¶„ì„ëœ í´ë” ìˆ˜: {len(folder_accuracy)}")
    print(f"âŒ ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤: {accuracy_results['valid_predictions'] - accuracy_results['correct_predictions']}")
    
    if folder_accuracy:
        best_folder = max(folder_accuracy.items(), key=lambda x: x[1]['accuracy'])
        worst_folder = min(folder_accuracy.items(), key=lambda x: x[1]['accuracy'])
        
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ í´ë”: {best_folder[0]} ({best_folder[1]['accuracy']*100:.2f}%)")
        print(f"ğŸ“‰ ìµœì € ì„±ëŠ¥ í´ë”: {worst_folder[0]} ({worst_folder[1]['accuracy']*100:.2f}%)")

def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    if len(sys.argv) != 2:
        print("âŒ ì‚¬ìš©ë²•: python result_calculator.py <CSV_íŒŒì¼_ê²½ë¡œ>")
        print("ì˜ˆì‹œ: python result_calculator.py results/ViT_g_14_result.csv")
        return
    
    csv_path = sys.argv[1]
    
    # CSV íŒŒì¼ ë¡œë“œ
    df = load_results_csv(csv_path)
    if df is None:
        return
    
    # ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
    print_detailed_analysis(df, csv_path)

if __name__ == "__main__":
    main()
