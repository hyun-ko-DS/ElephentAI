"""
ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì‹œìŠ¤í…œ
=======================

ëª©ì :
    train í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ CLIP ëª¨ë¸ë¡œ ì„ë² ë”©í•˜ì—¬ ë²¡í„°í™”í•˜ê³ , 
    numpy ë°°ì—´ í˜•íƒœë¡œ ì €ì¥í•˜ì—¬ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì…ë ¥ ìŠ¤í™:
---------
1. IMAGE_DIR: "train" - ì„ë² ë”©ì„ ìƒì„±í•  ì´ë¯¸ì§€ë“¤ì´ ìˆëŠ” í´ë”
2. MODEL_NAME: 'openai/clip-vit-large-patch14' - ì‚¬ìš©í•  CLIP ëª¨ë¸
3. BATCH_SIZE: 16 - í•œ ë²ˆì— ì²˜ë¦¬í•  ì´ë¯¸ì§€ ê°œìˆ˜ (GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
4. RECURSIVE_SCAN: False - í•˜ìœ„ í´ë”ê¹Œì§€ ìŠ¤ìº”í• ì§€ ì—¬ë¶€
5. ALLOWED_EXTS: {".jpg", ".jpeg", ".png", ".webp", ".bmp"} - ì§€ì› ì´ë¯¸ì§€ í™•ì¥ì

ì¶œë ¥ ìŠ¤í™:
---------
1. OUT_FEATURES: "embeddings/train_features_base_patch16.npy" - ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„°ë“¤ (N x 768)
2. OUT_PATHS: "embeddings/train_paths_base_patch16.npy" - ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë“¤ (Nê°œ)

ì‚¬ìš©ë²•:
------
python embeddings_train.py

ì£¼ì˜ì‚¬í•­:
---------
- GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•´ì•¼ í•©ë‹ˆë‹¤ (BATCH_SIZE ì¡°ì • í•„ìš”ì‹œ)
- train í´ë”ì— ì´ë¯¸ì§€ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
- ì²« ì‹¤í–‰ ì‹œ CLIP ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤
"""

import os
import time
from PIL import Image, UnidentifiedImageError
import numpy as np
from tqdm import tqdm
import torch
from transformers import CLIPProcessor, CLIPModel

# ==================== ì‹œìŠ¤í…œ ì„¤ì • ====================
IMAGE_DIR: str = "train"                    # ì„ë² ë”©ì„ ìƒì„±í•  ì´ë¯¸ì§€ í´ë”
OUT_FEATURES: str = "embeddings/train_features_large_patch14.npy"  # ì„ë² ë”© ë²¡í„° ì €ì¥ íŒŒì¼
OUT_PATHS: str = "embeddings/train_paths_large_patch14.npy"        # ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥ íŒŒì¼

# CLIP ëª¨ë¸ ì„ íƒ (ì£¼ì„ ì²˜ë¦¬ëœ ëª¨ë¸ë“¤ë„ ì‚¬ìš© ê°€ëŠ¥)
# MODEL_NAME: str = "openai/clip-vit-base-patch32"      # ê¸°ë³¸ ëª¨ë¸ (512ì°¨ì›)
# MODEL_NAME: str = 'openai/clip-vit-base-patch16'      # ê¸°ë³¸ ëª¨ë¸ (512ì°¨ì›)
MODEL_NAME: str = 'openai/clip-vit-large-patch14'      # ëŒ€í˜• ëª¨ë¸ (768ì°¨ì›, ë” ì •í™•í•¨)
# MODEL_NAME: str = 'openai/clip-vit-large-patch14-336' # ëŒ€í˜• ëª¨ë¸ (768ì°¨ì›, 336x336 í•´ìƒë„)
# MODEL_NAME: str = 'openai/clip-vit-huge-patch14-224' 

BATCH_SIZE: int = 32                       # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
RECURSIVE_SCAN: bool = True                # í•˜ìœ„ í´ë”ê¹Œì§€ ìŠ¤ìº”í• ì§€ ì—¬ë¶€
ALLOWED_EXTS: set = {".jpg", ".jpeg", ".png", ".webp", ".bmp"}  # ì§€ì› ì´ë¯¸ì§€ í™•ì¥ì

# ==================== í•µì‹¬ í•¨ìˆ˜ë“¤ ====================

def list_images(root: str, recursive: bool = True) -> list[str]:
    """
    ì§€ì •ëœ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì˜ ê²½ë¡œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        root (str): ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ë£¨íŠ¸ í´ë” ê²½ë¡œ
        recursive (bool): í•˜ìœ„ í´ë”ê¹Œì§€ ìŠ¤ìº”í• ì§€ ì—¬ë¶€
    
    ì¶œë ¥:
        list[str]: ì´ë¯¸ì§€ íŒŒì¼ë“¤ì˜ ì ˆëŒ€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ë¨)
    
    ì˜ˆì‹œ:
        >>> list_images("used", recursive=False)
        ['used/img1.jpg', 'used/img2.png', ...]
    """
    paths: list[str] = []
    
    if recursive:
        # í•˜ìœ„ í´ë”ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”
        for dirpath, _, filenames in os.walk(root):
            for filename in filenames:
                if os.path.splitext(filename)[1].lower() in ALLOWED_EXTS:
                    full_path: str = os.path.join(dirpath, filename)
                    paths.append(full_path)
    else:
        # í˜„ì¬ í´ë”ë§Œ ìŠ¤ìº”
        for filename in os.listdir(root):
            if os.path.splitext(filename)[1].lower() in ALLOWED_EXTS:
                full_path: str = os.path.join(root, filename)
                paths.append(full_path)
    
    # íŒŒì¼ëª… ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
    paths.sort()
    return paths

def load_model(model_name: str, device: torch.device) -> tuple[CLIPModel, CLIPProcessor]:
    """
    CLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        model_name (str): Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” CLIP ëª¨ë¸ëª…
        device (torch.device): GPU ë˜ëŠ” CPU ë””ë°”ì´ìŠ¤
    
    ì¶œë ¥:
        tuple[CLIPModel, CLIPProcessor]: CLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ê°ì²´
    
    ì˜ˆì‹œ:
        >>> model, processor = load_model("openai/clip-vit-large-patch14", torch.device("cuda"))
        >>> print(type(model))  # <class 'transformers.models.clip.modeling_clip.CLIPModel'>
    """
    # ì‚¬ì „ í›ˆë ¨ëœ CLIP ëª¨ë¸ ë¡œë“œ
    model: CLIPModel = CLIPModel.from_pretrained(model_name).to(device)
    
    # CLIP í”„ë¡œì„¸ì„œ ë¡œë“œ (ì´ë¯¸ì§€ ì „ì²˜ë¦¬ìš©)
    processor: CLIPProcessor = CLIPProcessor.from_pretrained(model_name)
    
    # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ, ë°°ì¹˜ ì •ê·œí™” ë“± ë¹„í™œì„±í™”)
    model.eval()
    
    return model, processor

def embed_batch(model: CLIPModel, processor: CLIPProcessor, 
                pil_images: list[Image.Image], device: torch.device) -> np.ndarray:
    """
    ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ CLIP ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
        model (CLIPModel): ë¡œë“œëœ CLIP ëª¨ë¸
        processor (CLIPProcessor): CLIP í”„ë¡œì„¸ì„œ
        pil_images (list[Image.Image]): PIL Image ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        device (torch.device): GPU ë˜ëŠ” CPU ë””ë°”ì´ìŠ¤
    
    ì¶œë ¥:
        np.ndarray: ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„°ë“¤ (ë°°ì¹˜í¬ê¸° x ì°¨ì›ìˆ˜)
                   - base ëª¨ë¸: 512ì°¨ì›
                   - large ëª¨ë¸: 768ì°¨ì›
    
    ì˜ˆì‹œ:
        >>> images = [Image.open("img1.jpg"), Image.open("img2.jpg")]
        >>> embeddings = embed_batch(model, processor, images, device)
        >>> print(embeddings.shape)  # (2, 768) for large model
    """
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        # ì´ë¯¸ì§€ë“¤ì„ CLIP ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        inputs = processor(images=pil_images, return_tensors="pt").to(device)
        
        # CLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        feats: torch.Tensor = model.get_image_features(**inputs)
        
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
        feats = feats / feats.norm(p=2, dim=-1, keepdim=True)
    
    # GPUì—ì„œ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜
    return feats.cpu().numpy().astype("float32")

def main() -> None:
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: train í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    ì…ë ¥: ì—†ìŒ
    
    ì¶œë ¥: ì—†ìŒ (íŒŒì¼ë¡œ ì €ì¥ë¨)
    
    ì²˜ë¦¬ ê³¼ì •:
    1. train í´ë” ì¡´ì¬ í™•ì¸
    2. GPU/CPU ë””ë°”ì´ìŠ¤ ì„ íƒ
    3. CLIP ëª¨ë¸ ë¡œë“œ
    4. ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    5. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
    6. ê²°ê³¼ë¥¼ numpy ë°°ì—´ë¡œ ì €ì¥
    
    ì˜ˆì™¸ ì²˜ë¦¬:
    - FileNotFoundError: IMAGE_DIRì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
    - RuntimeError: ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
    - UnidentifiedImageError: ì†ìƒëœ ì´ë¯¸ì§€ íŒŒì¼
    - OSError: íŒŒì¼ ì½ê¸° ì˜¤ë¥˜
    """
    start_time: float = time.time()
    
    # 1. train í´ë” ì¡´ì¬ í™•ì¸
    if not os.path.isdir(IMAGE_DIR):
        raise FileNotFoundError(f"IMAGE_DIR not found: {IMAGE_DIR}")
    
    # 2. GPU/CPU ë””ë°”ì´ìŠ¤ ì„ íƒ
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  Device: {device}")
    
    # 3. CLIP ëª¨ë¸ ë¡œë“œ
    print("ï¿½ï¿½ CLIP ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, processor = load_model(MODEL_NAME, device)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 4. ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    print("ğŸ” ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    img_paths: list[str] = list_images(IMAGE_DIR, recursive=RECURSIVE_SCAN)
    
    if not img_paths:
        raise RuntimeError(f"No images found under: {IMAGE_DIR}")
    
    print(f"ï¿½ï¿½ ì´ {len(img_paths)}ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬")
    
    # 5. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
    features: list[np.ndarray] = []        # ì„ë² ë”© ë²¡í„°ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    kept_paths: list[str] = []             # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê²½ë¡œë“¤
    batch: list[Image.Image] = []          # í˜„ì¬ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ë“¤
    batch_paths: list[str] = []            # í˜„ì¬ ë°°ì¹˜ì˜ ê²½ë¡œë“¤
    
    print(f"âš¡ ì„ë² ë”© ìƒì„± ì‹œì‘ (batch_size={BATCH_SIZE}, device={device})")
    
    # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì´ë¯¸ì§€ ì²˜ë¦¬
    for img_path in tqdm(img_paths, desc="ì´ë¯¸ì§€ ì²˜ë¦¬"):
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° RGB ë³€í™˜
            img: Image.Image = Image.open(img_path).convert("RGB")
            batch.append(img)
            batch_paths.append(img_path)
            
            # ë°°ì¹˜ê°€ ê°€ë“ ì°¼ì„ ë•Œ ì„ë² ë”© ìƒì„±
            if len(batch) == BATCH_SIZE:
                feats: np.ndarray = embed_batch(model, processor, batch, device)
                features.append(feats)
                kept_paths.extend(batch_paths)
                
                # ë°°ì¹˜ ì´ˆê¸°í™”
                batch, batch_paths = [], []
                
        except (UnidentifiedImageError, OSError) as e:
            # ì†ìƒëœ ì´ë¯¸ì§€ë‚˜ ì½ê¸° ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°
            print(f"âš ï¸  [SKIP] ë¡œë“œ ì‹¤íŒ¨: {img_path} ({e})")
    
    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ (BATCH_SIZEë³´ë‹¤ ì ì€ ê²½ìš°)
    if batch:
        feats: np.ndarray = embed_batch(model, processor, batch, device)
        features.append(feats)
        kept_paths.extend(batch_paths)
    
    # 6. ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    if features:
        feats_all: np.ndarray = np.vstack(features)
    else:
        # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´ ìƒì„±
        embedding_dim: int = 512 if "base" in MODEL_NAME else 768
        feats_all: np.ndarray = np.zeros((0, embedding_dim), dtype="float32")
    
    # 7. ê²°ê³¼ë¥¼ numpy ë°°ì—´ë¡œ ì €ì¥
    print("ğŸ’¾ ì„ë² ë”© ì €ì¥ ì¤‘...")
    np.save(OUT_FEATURES, feats_all)
    np.save(OUT_PATHS, np.array(kept_paths, dtype=object))
    
    # 8. ì™„ë£Œ ì •ë³´ ì¶œë ¥
    elapsed_time: float = time.time() - start_time
    print("âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
    print(f" ğŸ“Š features: {OUT_FEATURES} {feats_all.shape}")
    print(f" ğŸ“ paths   : {OUT_PATHS} {len(kept_paths)}ê°œ")
    print(f" â±ï¸  ê²½ê³¼ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    
    # 9. ì„±ê³µë¥  ê³„ì‚°
    success_rate: float = (len(kept_paths) / len(img_paths)) * 100
    print(f" ğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}% ({len(kept_paths)}/{len(img_paths)})")

# ==================== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ====================

if __name__ == "__main__":
    try:
        main()
        print("\nì„ë² ë”© ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ì´ì œ train í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì´ CLIP ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("   1. train í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€")
        print("   2. train í´ë”ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ”ì§€")
        print("   3. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ (BATCH_SIZE ì¡°ì • í•„ìš”ì‹œ)")
        print("   4. ì¸í„°ë„· ì—°ê²°ì´ ì•ˆì •ì ì¸ì§€ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)")